{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minimodel import data, model_builder, model_trainer, metrics\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda')\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "data_path = '../data'\n",
    "weight_path = './checkpoints/fullmodel'\n",
    "result_path = './save_results/outputs'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fev_all = []\n",
    "for mouse_id in range(6):\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    fev_all.append(dat['fev'])\n",
    "\n",
    "fev_all = np.hstack(fev_all)\n",
    "print(f'{np.sum(fev_all>0.15)}/{len(fev_all)} neurons have FEV > 0.15')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "mouse_id = 1\n",
    "\n",
    "# load images\n",
    "img = data.load_images(data_path, mouse_id, file=os.path.join(data_path, data.img_file_name[mouse_id]))\n",
    "\n",
    "# load neurons\n",
    "fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "# split train and validation set\n",
    "itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "\n",
    "# normalize spks\n",
    "spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "\n",
    "ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "input_Ly, input_Lx = img_test.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spks_rep_all = np.stack(spks_rep_all)\n",
    "spks_rep_mean = np.mean(spks_rep_all, axis=1)\n",
    "print(spks_rep_mean.shape, spks_rep_mean.min(), spks_rep_mean.max())\n",
    "data_dict['spks_test_mean_example'] = spks_rep_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayers = 4\n",
    "nconv1 = 192\n",
    "nconv2 = 192\n",
    "suffix = ''\n",
    "model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels)\n",
    "\n",
    "weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "print('model path: ', model_path)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)\n",
    "\n",
    "# test model\n",
    "test_pred = model_trainer.test_epoch(model, img_test)\n",
    "print('test_pred: ', test_pred.shape, test_pred.min(), test_pred.max())\n",
    "\n",
    "test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "threshold = 0.15\n",
    "print(f'filtering neurons with FEV > {threshold}')\n",
    "valid_idxes = np.where(test_fev > threshold)[0]\n",
    "print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "\n",
    "\n",
    "data_dict['fev_example'] = test_fev\n",
    "data_dict['feve_example'] = test_feve\n",
    "data_dict['spks_pred_test_example'] = test_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 1d FEV versus FEVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fev_all = []\n",
    "feve_all = []\n",
    "for mouse_id in range(6):\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    fev_all.append(dat['fev'])\n",
    "    feve_all.append(dat['feve_depth'][3])\n",
    "\n",
    "fev_all = np.hstack(fev_all)\n",
    "feve_all = np.hstack(feve_all)\n",
    "\n",
    "data_dict['feve_all'] = feve_all\n",
    "data_dict['fev_all'] = fev_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 1e (5k vs 30k train images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "seed = 1\n",
    "nlayers = 4\n",
    "nconv1 = 192\n",
    "nconv2 = 192\n",
    "nmouse = 6\n",
    "nstim_list = [5000, 30000]\n",
    "# n_max_neurons = len(valid_idxes)\n",
    "feve_nstims = np.zeros((nmouse, len(nstim_list)))\n",
    "for mouse_id in range(nmouse):\n",
    "    # load images\n",
    "    img = data.load_images(data_path, mouse_id, file=os.path.join(data_path, data.img_file_name[mouse_id]))\n",
    "    nimg, Ly, Lx = img.shape\n",
    "    print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "\n",
    "    # load neurons\n",
    "    fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "    spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "    n_stim, n_max_neurons = spks.shape\n",
    "    itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "    # normalize spks\n",
    "    spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "    ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "    spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "    img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "    input_Ly, input_Lx = img_test.shape[-2:]\n",
    "    \n",
    "    for i, n_stim in enumerate(nstim_list):\n",
    "        if n_stim  > len(itrain): n_stim = len(itrain)\n",
    "        \n",
    "        suffix = f'nstims_{n_stim}'\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix)\n",
    "\n",
    "        weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "        if not os.path.exists(weight_path):\n",
    "            os.makedirs(weight_path)\n",
    "        model_path = os.path.join(weight_path, model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "        # print('test_pred: ', test_pred.shape, test_pred.min(), test_pred.max())\n",
    "\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        threshold = 0.15\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print('FEVE (test): ', np.mean(test_feve[valid_idxes]))\n",
    "\n",
    "        feve_nstims[mouse_id, i] = np.mean(test_feve[valid_idxes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['5k_30k_feve'] = feve_nstims"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 1e (FEVE distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fev_all = []\n",
    "feve_all = []\n",
    "for mouse_id in range(6):\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    fev_all.append(dat['fev'])\n",
    "    feve_all.append(dat['feve_depth'][3])\n",
    "fev_all = np.hstack(fev_all)\n",
    "feve_all = np.hstack(feve_all)\n",
    "data_dict['feve_all_mice'] = feve_all[fev_all>0.15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 1f (performance change with model depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "feve_depth_all = np.zeros((nmouse, 4))\n",
    "feve_LN_all = np.zeros(nmouse)\n",
    "for mouse_id in range(6):\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    fev = dat['fev']\n",
    "    feve_depth = dat['feve_depth']\n",
    "    feve_depth_all[mouse_id] = feve_depth[:, fev>0.15].mean(axis=1)\n",
    "    feve_LN_all[mouse_id] = dat['LNmodel_feve_all'][fev>0.15].mean()\n",
    "data_dict['feve_our_model'] = feve_depth_all\n",
    "data_dict['feve_LN_model'] = feve_LN_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Lurz_model_train_test.ipynb first\n",
    "lurz_feve_all = np.load(os.path.join(result_path, 'lurz_feve_all.npy'))\n",
    "data_dict['feve_lurz_model'] = lurz_feve_all\n",
    "print(data_dict['feve_lurz_model'].mean(axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 1g (visualize readout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "mouse_id = 1\n",
    "\n",
    "# load neurons\n",
    "ineur = np.arange(0, data.NNs[mouse_id]) #np.arange(0, n_neurons, 5)\n",
    "input_Ly, input_Lx = 66, 130\n",
    "\n",
    "nlayers = 2\n",
    "nconv1 = 192\n",
    "nconv2 = 192\n",
    "model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels)\n",
    "\n",
    "weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "print('model path: ', model_path)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wc = model.readout.Wc.detach().cpu().numpy().squeeze()\n",
    "# # change model Wx and Wy\n",
    "Wx = model.readout.Wx.detach().cpu().numpy()\n",
    "Wy = model.readout.Wy.detach().cpu().numpy()\n",
    "# outer product of Wx and Wy\n",
    "Wxy = np.einsum('icj,ick->ijk', Wy, Wx)\n",
    "\n",
    "# rfsize from the Wxy\n",
    "from minimodel.utils import weight_bandwidth\n",
    "NN = Wxy.shape[0]\n",
    "bandwidth_Wx = np.zeros(NN)\n",
    "bandwidth_Wy = np.zeros(NN)\n",
    "for i in range(NN):\n",
    "    bandwidth_Wx[i] = weight_bandwidth(Wx[i, 0, :])\n",
    "    bandwidth_Wy[i] = weight_bandwidth(Wy[i, 0, :])\n",
    "rf_size = bandwidth_Wx * bandwidth_Wy\n",
    "print(f'average rf size: {np.mean(rf_size):.2f}')\n",
    "\n",
    "data_dict['Wxy_example'] = Wxy\n",
    "data_dict['Wx_example'] = Wx\n",
    "data_dict['Wy_example'] = Wy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 1h (distribution of pooling area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "mouse_list = [0, 1, 2, 3, 4, 5]\n",
    "nconv1 = 192\n",
    "nconv2 = 192\n",
    "nl = 2\n",
    "rfsize_all = []\n",
    "for n, mouse_id in enumerate(mouse_list):\n",
    "    # load neurons\n",
    "    ineur = np.arange(0, data.NNs[mouse_id]) \n",
    "    input_Ly, input_Lx = 66, 130\n",
    "\n",
    "    nlayers = 2\n",
    "    nconv1 = 192\n",
    "    nconv2 = 192\n",
    "    suffix = ''\n",
    "    if mouse_id == 5: suffix += f'xrange_{xrange_max}'\n",
    "    model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "    model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, suffix=suffix)\n",
    "\n",
    "    weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "    if not os.path.exists(weight_path):\n",
    "        os.makedirs(weight_path)\n",
    "    model_path = os.path.join(weight_path, model_name)\n",
    "    print('model path: ', model_path)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model', model_path)\n",
    "\n",
    "    Wc = model.readout.Wc.detach().cpu().numpy().squeeze()\n",
    "    # # change model Wx and Wy\n",
    "    Wx = model.readout.Wx.detach().cpu().numpy()\n",
    "    Wy = model.readout.Wy.detach().cpu().numpy()\n",
    "    # outer product of Wx and Wy\n",
    "    Wxy = np.einsum('icj,ick->ijk', Wy, Wx)\n",
    "    print(Wxy.shape, Wc.shape)\n",
    "    print(Wc.shape, Wc.min(), Wc.max())\n",
    "\n",
    "    # rfsize from the Wxy\n",
    "    from minimodel.utils import weight_bandwidth\n",
    "    NN = Wxy.shape[0]\n",
    "    bandwidth_Wx = np.zeros(NN)\n",
    "    bandwidth_Wy = np.zeros(NN)\n",
    "    for i in range(NN):\n",
    "        bandwidth_Wx[i] = weight_bandwidth(Wx[i, 0, :])\n",
    "        bandwidth_Wy[i] = weight_bandwidth(Wy[i, 0, :])\n",
    "    rf_size = bandwidth_Wx * bandwidth_Wy\n",
    "    print(f'average rf size: {np.mean(rf_size):.2f}')\n",
    "    rfsize_all.append(rf_size)\n",
    "rfsize_all = np.hstack(rfsize_all)\n",
    "print(rfsize_all.shape)\n",
    "data_dict['rfsize_all'] = rfsize_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# monkey"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure 1i (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dat = np.load(os.path.join(data_path, 'monkeyv1_cadena_2019.npz'))\n",
    "images = dat['images']\n",
    "responses = dat['responses']\n",
    "real_responses = dat['real_responses']\n",
    "test_images = dat['test_images']\n",
    "test_responses = dat['test_responses']\n",
    "test_real_responses = dat['test_real_responses']\n",
    "train_idx = dat['train_idx']\n",
    "val_idx = dat['val_idx']\n",
    "repetitions = dat['repetitions']\n",
    "monkey_ids = dat['subject_id']\n",
    "image_ids = dat['image_ids']\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_img, _, Ly, Lx = images.shape\n",
    "np.random.seed(42)\n",
    "iselect = np.random.choice(n_img, 10, replace=False)\n",
    "fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax[i//5, i%5].imshow(images[iselect[i]].mean(0), cmap='gray')\n",
    "    ax[i//5, i%5].axis('off')\n",
    "plt.savefig('monkeyv1_images.pdf', dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure 1j-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dat = np.load(os.path.join(data_path, 'monkeyv1_cadena_2019.npz'))\n",
    "images = dat['images']\n",
    "responses = dat['responses']\n",
    "real_responses = dat['real_responses']\n",
    "test_images = dat['test_images']\n",
    "test_responses = dat['test_responses']\n",
    "test_real_responses = dat['test_real_responses']\n",
    "train_idx = dat['train_idx']\n",
    "val_idx = dat['val_idx']\n",
    "repetitions = dat['repetitions']\n",
    "monkey_ids = dat['subject_id']\n",
    "image_ids = dat['image_ids']\n",
    "\n",
    "# normalize responses\n",
    "responses_nan = np.where(real_responses, responses, np.nan)\n",
    "resp_std = np.nanstd(responses_nan, axis=0)\n",
    "responses = responses / resp_std\n",
    "test_responses = test_responses / resp_std\n",
    "\n",
    "test_responses_nan = np.where(test_real_responses, test_responses, np.nan)\n",
    "print('test_responses_nan: ', test_responses_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayers = 2\n",
    "nconv1 = 192\n",
    "nconv2 = 192\n",
    "Lx, Ly = 80, 80\n",
    "model, in_channels = model_builder.build_model(NN=166, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, input_Lx=Lx, input_Ly=Ly)\n",
    "suffix = ''\n",
    "model_name = model_builder.create_model_name('monkeyV1', '2019', n_layers=nlayers, in_channels=in_channels, suffix=suffix)\n",
    "weight_path = os.path.join(weight_path, 'fullmodel', 'monkeyV1')\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "print('model path: ', model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)\n",
    "model = model.to(device)\n",
    "test_images = torch.from_numpy(test_images).to(device)\n",
    "spks_pred_test = model_trainer.test_epoch(model, test_images)\n",
    "print('spks_pred_test: ', spks_pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spks_test_mean = np.nanmean(test_responses_nan, axis=0)\n",
    "dat = np.load(os.path.join(result_path, f'fullmodel_monkey_results.npz'), allow_pickle=True)\n",
    "\n",
    "data_dict['monkey_fev'] = dat['fev_all']\n",
    "data_dict['monkey_feve'] = dat['feve_depth'][1]\n",
    "data_dict['monkey_spks_test_mean'] = spks_test_mean\n",
    "data_dict['monkey_spks_pred_test'] = spks_pred_test\n",
    "data_dict['monkey_LNmodel_feve'] = dat['LNmodel_feve_all'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure 1l (FEVE change with depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.load(f'outputs/fullmodel_monkey_results.npz', allow_pickle=True)\n",
    "data_dict['monkey_id'] = dat['monkey_ids']\n",
    "monkey_eve = dat['feve_depth']\n",
    "imonkey1 = np.where(data_dict['monkey_id']==4)[0]\n",
    "imonkey2 = np.where(data_dict['monkey_id']==34)[0]\n",
    "separate_eve = np.zeros((2, 4))\n",
    "separate_eve[0] = np.mean(monkey_eve[:, imonkey1], axis=1)\n",
    "separate_eve[1] = np.mean(monkey_eve[:, imonkey2], axis=1)\n",
    "data_dict['monkey_depth_eve'] = monkey_eve.T\n",
    "data_dict['monkey_feve'] = dat['feve_depth'][1]\n",
    "data_dict['monkey_fev'] = dat['fev_all']\n",
    "print(monkey_eve.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG feve\n",
    "vgg_eve = np.load(os.path.join(result_path, 'Cadena_vgg_feve.npy'))\n",
    "print(vgg_eve.mean())\n",
    "\n",
    "data_dict['vgg_eve'] = vgg_eve\n",
    "# data_dict['fullmodel_eve'] = fullmodel_eve\n",
    "vgg_eve_monkey1 = vgg_eve[imonkey1].mean()\n",
    "vgg_eve_monkey2 = vgg_eve[imonkey2].mean()\n",
    "print(vgg_eve_monkey1, vgg_eve_monkey2, vgg_eve.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure 1m-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayers = 2\n",
    "nconv1 = 192\n",
    "nconv2 = 192\n",
    "Lx, Ly = 80, 80\n",
    "model, in_channels = model_builder.build_model(NN=166, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, input_Lx=Lx, input_Ly=Ly)\n",
    "suffix = ''\n",
    "model_name = model_builder.create_model_name('monkeyV1', '2019', n_layers=nlayers, in_channels=in_channels, suffix=suffix)\n",
    "weight_path = os.path.join(weight_path, 'fullmodel', 'monkeyV1')\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "print('model path: ', model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)\n",
    "\n",
    "Wc = model.readout.Wc.detach().cpu().numpy().squeeze()\n",
    "# # change model Wx and Wy\n",
    "Wx = model.readout.Wx.detach().cpu().numpy()\n",
    "Wy = model.readout.Wy.detach().cpu().numpy()\n",
    "# outer product of Wx and Wy\n",
    "Wxy = np.einsum('icj,ick->ijk', Wy, Wx)\n",
    "\n",
    "# rfsize from the Wxy\n",
    "from minimodel.utils import weight_bandwidth\n",
    "NN = Wxy.shape[0]\n",
    "bandwidth_Wx = np.zeros(NN)\n",
    "bandwidth_Wy = np.zeros(NN)\n",
    "for i in range(NN):\n",
    "    bandwidth_Wx[i] = weight_bandwidth(Wx[i, 0, :])\n",
    "    bandwidth_Wy[i] = weight_bandwidth(Wy[i, 0, :])\n",
    "rf_size = bandwidth_Wx * bandwidth_Wy\n",
    "print(f'average rf size: {np.mean(rf_size):.2f}')\n",
    "data_dict['monkey_rfsize'] = rf_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['monkey_Wxy'] = Wxy\n",
    "data_dict['monkey_Wx'] = Wx\n",
    "data_dict['monkey_Wy'] = Wy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_dict\n",
    "np.savez(f'figure1_results.npz', **data_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import figure1\n",
    "dat = np.load('figure1_results.npz', allow_pickle=True)\n",
    "save_path = './outputs'\n",
    "figure1.figure1(dat, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
