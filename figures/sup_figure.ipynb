{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from minimodel import data, model_builder, metrics, model_trainer\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "data_path = '../notebooks/data'\n",
    "weight_path = './checkpoints/fullmodel'\n",
    "result_path = './save_results/outputs'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 1: retinotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "mouse_id = 0\n",
    "xpos_all = []\n",
    "ypos_all = []\n",
    "xpos_visual_all = []\n",
    "ypos_visual_all = []\n",
    "x_pixel_ratio = 0.75\n",
    "y_pixel_ratio = 0.5\n",
    "fev_all = []\n",
    "for mouse_id in range(6):\n",
    "    fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "    spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "    xpos_all.append(xpos)\n",
    "    ypos_all.append(ypos)\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    from minimodel.utils import weight_bandwidth\n",
    "    Wx = dat['fullmodel_Wx']\n",
    "    Wy = dat['fullmodel_Wy']\n",
    "    NN = Wx.shape[0]\n",
    "    bandwidth_Wx = np.zeros(NN)\n",
    "    bandwidth_Wy = np.zeros(NN)\n",
    "    centerpos_Wx = np.zeros(NN)\n",
    "    centerpos_Wy = np.zeros(NN)\n",
    "    for i in range(NN):\n",
    "        bandwidth_Wx[i], centerpos_Wx[i] = weight_bandwidth(Wx[i, :], return_peak=True)\n",
    "        bandwidth_Wy[i], centerpos_Wy[i] = weight_bandwidth(Wy[i, :], return_peak=True)\n",
    "    xpos_model = np.argmax(Wx.squeeze(), axis=1) \n",
    "    ypos_model = np.argmax(Wy.squeeze(), axis=1) \n",
    "    xpos_visual = centerpos_Wx*2*(270/264) - 135 # 0 is at the center, so it should be 135 pixels\n",
    "    ypos_visual = centerpos_Wy*2*(65/66) - 32.5 # vertical visual range is 65, so it sould be (66/65) pixels per degree\n",
    "    if mouse_id == 5: xpos_visual += (46*270/264) # xrange of mouse 6 is 46-176\n",
    "    xpos = xpos / x_pixel_ratio\n",
    "    ypos = ypos / y_pixel_ratio\n",
    "    if mouse_id == 1:\n",
    "        idx_up = np.where(xpos>325/ x_pixel_ratio)[0]\n",
    "        idx_down = np.where(xpos<=325/ x_pixel_ratio)[0]\n",
    "        ymax = ypos[idx_up].max()\n",
    "        xmax, xmin = xpos[idx_up].max(), xpos[idx_up].min()\n",
    "        ypos[idx_up] = ymax - ypos[idx_up] +300 # + ymax\n",
    "    xpos_visual_all.append(xpos_visual)\n",
    "    ypos_visual_all.append(ypos_visual)\n",
    "    fev_all.append(dat['fev'])\n",
    "data_dict['xpos_all'] = np.array(xpos_all)\n",
    "data_dict['ypos_all'] = np.array(ypos_all)\n",
    "data_dict['xpos_visual_all'] = np.array(xpos_visual_all)\n",
    "data_dict['ypos_visual_all'] = np.array(ypos_visual_all)\n",
    "data_dict['fev_all'] = np.array(fev_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "db = []\n",
    "\n",
    "db.append({'mname': 'L1_A5', 'datexp': '2023_01_27', 'blk':'3', 'stim':'short3'})\n",
    "db.append({'mname': 'L1_A1', 'datexp': '2023_03_27', 'blk':'1', 'stim':'short3'})\n",
    "db.append({'mname': 'FX9', 'datexp': '2023_05_02', 'blk':'2', 'stim':'short3'})\n",
    "db.append({'mname': 'FX10', 'datexp': '2023_05_02', 'blk':'1', 'stim':'short3'})\n",
    "db.append({'mname': 'FX8', 'datexp': '2023_05_02', 'blk':'1', 'stim':'short3'})\n",
    "db.append({'mname': 'FX20', 'datexp': '2023_09_08', 'blk':'2','stim':'nat15k'})\n",
    "\n",
    "iregion_all = []\n",
    "iarea_all = []\n",
    "xy_all = []\n",
    "out_all = []\n",
    "jxy_all = []\n",
    "\n",
    "for i, mouse in enumerate(db[:6]):\n",
    "    aligned_path = Path(os.path.join(result_path, 'Ret_maps'))\n",
    "    aligned_path = aligned_path.joinpath(f\"{db[i]['mname']}_{db[i]['datexp']}_{db[i]['blk']}_{db[i]['stim']}.npz\")\n",
    "\n",
    "    m = np.load(aligned_path, allow_pickle=True)\n",
    "    iregion = m['iregion']\n",
    "    iarea = m['iarea']\n",
    "    xy = m['xy']\n",
    "    out = m['out']\n",
    "    jxy = m['jxy']\n",
    "    iregion_all.append(iregion)\n",
    "    iarea_all.append(iarea)\n",
    "    xy_all.append(xy)\n",
    "    out_all.append(out)\n",
    "    jxy_all.append(jxy)\n",
    "data_dict['iregion'] = np.array(iregion_all)\n",
    "data_dict['iarea'] = np.array(iarea_all)\n",
    "data_dict['xy'] = np.array(xy_all)  \n",
    "data_dict['out'] = np.array(out_all)\n",
    "data_dict['jxy'] = np.array(jxy_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sup_figure\n",
    "from fig_utils import *\n",
    "save_path = './outputs'\n",
    "sup_figure.figure1(data_dict, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 2: signal variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example neuron Trial 1 vs trial 2\n",
    "mouse_id = 1\n",
    "fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "# spks, spks_rep, itrain, ival, fev, istim_test = load_activity(file_path=mouse_file_paths[mouse_id], mouse_id=mouse_id)\n",
    "# split train and validation set\n",
    "itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "# normalize spks\n",
    "spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "fev_all = metrics.fev(spks_rep_all)\n",
    "\n",
    "ineurons = [2550, 5020, 264]\n",
    "spks_rep = [spks_rep_all[i][:,ineurons] for i in range(len(spks_rep_all))]\n",
    "data_dict['example_repeats'] = np.stack(spks_rep)\n",
    "data_dict['example_fev'] = fev_all[ineurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal variance distribution mouse\n",
    "fev_all = []\n",
    "for mouse_id in range(6):\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    fev_all.append(dat['fev'])\n",
    "fev_all = np.hstack(fev_all)\n",
    "data_dict['fev_all'] = fev_all\n",
    "print(np.mean(fev_all))\n",
    "print(np.mean(fev_all[fev_all>0.15]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dat = np.load(os.path.join(data_path, 'monkeyv1_cadena_2019.npz'))\n",
    "images = dat['images']\n",
    "responses = dat['responses']\n",
    "real_responses = dat['real_responses']\n",
    "test_images = dat['test_images']\n",
    "test_responses = dat['test_responses']\n",
    "test_real_responses = dat['test_real_responses']\n",
    "train_idx = dat['train_idx']\n",
    "val_idx = dat['val_idx']\n",
    "repetitions = dat['repetitions']\n",
    "monkey_ids = dat['subject_id']\n",
    "image_ids = dat['image_ids']\n",
    "\n",
    "# normalize responses\n",
    "responses_nan = np.where(real_responses, responses, np.nan)\n",
    "resp_std = np.nanstd(responses_nan, axis=0)\n",
    "responses = responses / resp_std\n",
    "test_responses = test_responses / resp_std\n",
    "\n",
    "test_responses_nan = np.where(test_real_responses, test_responses, np.nan)\n",
    "print('test_responses_nan: ', test_responses_nan.shape)\n",
    "\n",
    "# spks_test_mean = np.nanmean(test_responses_nan, axis=0)\n",
    "# print(spks_test_mean.shape)\n",
    "dat = np.load(os.path.join(result_path, f'fullmodel_monkey_results.npz'), allow_pickle=True)\n",
    "data_dict['monkey_fev_all'] = dat['fev_all']\n",
    "ineurons = [51, 105, 86]\n",
    "data_dict['monkey_example_repeats'] = test_responses_nan[:, :, ineurons]\n",
    "data_dict['monkey_example_fev'] = dat['fev_all'][ineurons]\n",
    "data_dict['monkey_example_reps'] = repetitions[ineurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ineurons = [51, 105, 86]\n",
    "data_dict['monkey_example_repeats'] = test_responses_nan[:, :, ineurons]\n",
    "data_dict['monkey_example_fev'] = dat['fev_all'][ineurons]\n",
    "data_dict['monkey_example_reps'] = repetitions[ineurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sup_figure\n",
    "from fig_utils import *\n",
    "save_path = './outputs'\n",
    "sup_figure.figure2(data_dict, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 4: Gabor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_id = 0\n",
    "server_path = '/home/carsen/dm11_cluster/fengtongd/Desktop/approxineuro'\n",
    "res_dict = {'fev': [], 'feve_minimodel': [], 'feve_gabor': [], 'mf': [], 'msigma': [], 'mtheta': [], 'cratio': [], 'feve_fullmodel': []}\n",
    "\n",
    "for mouse_id in range(6):\n",
    "    # load images\n",
    "    data_path = os.path.join(server_path, 'data')  \n",
    "\n",
    "    # load neurons\n",
    "    fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "    spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "    n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "    # split train and validation set\n",
    "    itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "    ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "    # normalize spks\n",
    "    spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "    spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "    spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "    ineurons = np.arange(data.NNs_valid[mouse_id])\n",
    "    # np.random.seed(42)\n",
    "    # ineurons = np.random.choice(ineurons, 100, replace=False)\n",
    "\n",
    "    fev_test = metrics.fev(spks_rep_all)\n",
    "    isort_neurons = np.argsort(fev_test)[::-1]\n",
    "    ineur = isort_neurons[ineurons]\n",
    "\n",
    "    print(spks.shape, spks_val.shape, len(spks_rep_all), spks_rep_all[0].shape)\n",
    "\n",
    "    spks = spks[:,ineur]\n",
    "    spks_val = spks_val[:,ineur]\n",
    "    spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "    print(spks.shape, spks_val.shape, len(spks_rep_all), spks_rep_all[0].shape)\n",
    "\n",
    "    if mouse_id == 5:\n",
    "        xrange_max = 176\n",
    "    else:\n",
    "        xrange_max = 130 \n",
    "    img_all = data.load_images(data_path, file=os.path.join(data_path, data.img_file_name[mouse_id]), xrange=[xrange_max-130, xrange_max], downsample=2)\n",
    "    nimg, Ly, Lx = img_all.shape\n",
    "    print('img: ', img_all.shape, img_all.min(), img_all.max())\n",
    "\n",
    "    n_stim = -1 # spks.shape[0]\n",
    "    n_neurons = -1\n",
    "\n",
    "    # generate random data\n",
    "    if n_stim > 0:\n",
    "        istims = np.random.choice(spks.shape[0], n_stim, replace=False)\n",
    "    else:\n",
    "        n_stim = spks.shape[0]\n",
    "        istims = np.arange(n_stim)\n",
    "    if n_neurons > 0:\n",
    "        ineurons = np.random.choice(spks.shape[1], n_neurons, replace=False)\n",
    "        X_test = [spks_rep_all[i][:,ineurons] for i in range(len(spks_rep_all))]\n",
    "    else:\n",
    "        n_neurons = spks.shape[1]\n",
    "        ineurons = np.arange(n_neurons)\n",
    "        X_test = spks_rep_all.copy()\n",
    "    X = spks[istims][:,ineurons]\n",
    "    img = img_all[istim_train][istims].transpose(1,2,0)\n",
    "    img_test = img_all[istim_test].transpose(1,2,0)\n",
    "    print(f'img: {img.shape}, X: {X.shape}')\n",
    "    Ly, Lx, _ = img.shape\n",
    "\n",
    "    # define gabor parameters\n",
    "    sigma = np.array([0.75, 1.25, 1.5, 2.5, 3.5, 4.5, 5.5])\n",
    "    f = np.array([0.1, 0.25, 0.5, 1, 2]) #[.01:.02:.13];\n",
    "    theta = np.arange(0, np.pi, np.pi/8) # [0, pi/8, pi/4, 3*pi/8];\n",
    "    ph = np.arange(0, 2*np.pi, np.pi/4) # [0, pi/4, pi/2, 3*pi/4, pi, 5*pi/4, 3*pi/2, 7*pi/4];\n",
    "    ar = np.array([1, 1.5, 2])\n",
    "    print(f'sigma: {sigma.shape}, f: {f.shape}, theta: {theta.shape}, ph: {ph.shape}, ar: {ar.shape}')\n",
    "\n",
    "    params = np.meshgrid(sigma, f, theta, ph, ar, indexing='ij')\n",
    "    n_gabors = params[0].size\n",
    "    print(f'number of gabors: {n_gabors}')\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        params[i] = np.expand_dims(params[i], axis=(-2,-1))\n",
    "        params[i] = torch.from_numpy(params[i].astype('float32'))\n",
    "    sigma, f, theta, ph, ar = params\n",
    "    print(f'sigma: {sigma.shape}, f: {f.shape}, theta: {theta.shape}, ph: {ph.shape}, ar: {ar.shape}')\n",
    "\n",
    "\n",
    "    result_dict = np.load(os.path.join(server_path, 'weights', 'gabor', f'gabor_params_{data.db[mouse_id][\"mname\"]}.npz'), allow_pickle=True)\n",
    "\n",
    "    xmax, ymax = result_dict['xmax'], result_dict['ymax']\n",
    "    ys, xs = np.meshgrid(np.arange(0,Ly), np.arange(0,Lx), indexing='ij')\n",
    "    ys, xs = torch.from_numpy(ys.astype('float32')), torch.from_numpy(xs.astype('float32'))\n",
    "    gmax = result_dict['gmax']\n",
    "    gabor_params = torch.zeros((5, n_neurons, 1, 1))\n",
    "    for i in range(len(gabor_params)):\n",
    "        gabor_params[i] = params[i].flatten()[gmax].reshape(n_neurons, 1, 1)\n",
    "    msigma, mf, mtheta, mph, mar = gabor_params\n",
    "    Amax = result_dict['Amax']\n",
    "    mu1 = torch.from_numpy(result_dict['mu1']).to(device)\n",
    "    mu2 = torch.from_numpy(result_dict['mu2']).to(device)\n",
    "    #  test\n",
    "    ym = torch.from_numpy(ymax.astype('float32')).unsqueeze(-1).unsqueeze(-1)\n",
    "    xm = torch.from_numpy(xmax.astype('float32')).unsqueeze(-1).unsqueeze(-1)\n",
    "    # print(f'ym: {ym.shape}, xm: {xm.shape}')\n",
    "    gabor_params = torch.zeros((5, n_neurons, 1, 1))\n",
    "    for i in range(len(gabor_params)):\n",
    "        gabor_params[i] = params[i].flatten()[gmax].reshape(n_neurons, 1, 1)\n",
    "    msigma, mf, mtheta, mph, mar = gabor_params\n",
    "    from minimodel.gabor import gabor_filter, eval_gabors\n",
    "    gabor_filters1 = gabor_filter(ys, xs, ym, xm, 1, msigma, mf, mtheta, mph, mar, is_torch=True).to(device).unsqueeze(-3)\n",
    "    gabor_filters2 = gabor_filter(ys, xs, ym, xm, 1, msigma, mf, mtheta, mph + np.pi/2, mar, is_torch=True).to(device).unsqueeze(-3)\n",
    "\n",
    "    # predict responses\n",
    "    ntest = len(istim_test)\n",
    "    resp_test1 = torch.zeros((n_neurons, ntest), dtype=torch.float32, device=device)\n",
    "    resp_test2 = torch.zeros((n_neurons, ntest), dtype=torch.float32, device=device)\n",
    "    eval_gabors(img_test, gabor_filters1, resp_test1, device=device, rectify=False)\n",
    "    eval_gabors(img_test, gabor_filters2, resp_test2, device=device, rectify=False)\n",
    "    resp_test2 = torch.sqrt(resp_test1**2 + resp_test2**2) # RMS for complex cell response\n",
    "    from torch.nn.functional import relu\n",
    "    resp_test2 = relu(resp_test2) # rectify\n",
    "    resp_test1 = relu(resp_test1) # rectify\n",
    "\n",
    "    c = torch.from_numpy(Amax).to(device)\n",
    "\n",
    "    rpred = ((resp_test1.T - mu1) * c[:,0] + (resp_test2.T - mu2) * c[:,1]) # (n_stim, n_neurons)\n",
    "    print(f'rpred: {rpred.shape}')\n",
    "\n",
    "    # test responses\n",
    "    train_mu = result_dict['train_mu']\n",
    "    train_std = result_dict['train_std']\n",
    "    X_test = [spks_rep_all[i][:,ineurons] for i in range(len(spks_rep_all))]\n",
    "    for i in range(len(X_test)):\n",
    "        X_test[i] -= train_mu\n",
    "        X_test[i] /= train_std\n",
    "\n",
    "    fev, feve_gabor = metrics.feve(X_test, rpred.cpu().numpy())\n",
    "    print(f'fev:{fev.mean():.3f}, feve:{feve_gabor.mean():.3f}')\n",
    "\n",
    "    cratio = Amax[:,1]/Amax.sum(axis=1)\n",
    "\n",
    "    params = [mf.cpu().numpy().squeeze(), msigma.cpu().numpy().squeeze(), mtheta.cpu().numpy().squeeze()]\n",
    "\n",
    "    # load fullmodel feve\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    # fullmodel_feve = dat['fullmodel_feve_all']\n",
    "    fullmodel_feve = dat['feve_depth'][3]\n",
    "    nn_all = len(fullmodel_feve)\n",
    "    valid_idxes = dat['valid_idxes']\n",
    "    fullmodel_feve = fullmodel_feve[ineur]\n",
    "\n",
    "    dat = np.load(os.path.join(result_path, f'minimodel_{data.mouse_names[mouse_id]}_result.npz'), allow_pickle=True)\n",
    "    minimodel_feve = np.inf * np.ones(nn_all)\n",
    "    minimodel_feve[valid_idxes] = dat['feve_all']\n",
    "    minimodel_feve = minimodel_feve[ineur]\n",
    "\n",
    "    print(fev.shape, fullmodel_feve.shape, minimodel_feve.shape, feve_gabor.shape)\n",
    "    print(cratio.shape)\n",
    "\n",
    "    res_dict['fev'].append(fev)\n",
    "    res_dict['feve_minimodel'].append(minimodel_feve)\n",
    "    res_dict['feve_gabor'].append(feve_gabor)\n",
    "    res_dict['mf'].append(params[0])\n",
    "    res_dict['msigma'].append(params[1])\n",
    "    res_dict['mtheta'].append(params[2])\n",
    "    res_dict['cratio'].append(cratio)\n",
    "    res_dict['feve_fullmodel'].append(fullmodel_feve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in res_dict.keys():\n",
    "    res_dict[key] = np.hstack(res_dict[key])\n",
    "    print(key, res_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['fev_gabor'] = res_dict['fev']\n",
    "data_dict['feve_minimodel_gabor'] = res_dict['feve_minimodel']\n",
    "data_dict['feve_gabor'] = res_dict['feve_gabor']\n",
    "data_dict['feve_fullmodel_gabor'] = res_dict['feve_fullmodel']\n",
    "data_dict['mf'] = res_dict['mf']\n",
    "data_dict['msigma'] = res_dict['msigma']\n",
    "data_dict['mtheta'] = res_dict['mtheta']\n",
    "data_dict['cratio'] = res_dict['cratio']          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sup_figure\n",
    "from fig_utils import *\n",
    "save_path = './outputs'\n",
    "sup_figure.figure_gabor(data_dict, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 5: same core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "# res_path = 'outputs/feve_same_core_1layer.npz'\n",
    "res_path = os.path.join(result_path, 'feve_same_core_1layer.npz')\n",
    "if os.path.exists(res_path):\n",
    "    dat = np.load(res_path)\n",
    "    feve_ds = dat['feve_ds']\n",
    "    feve_our = dat['feve_our']\n",
    "else:\n",
    "    feve_ds = np.zeros(nmouse)\n",
    "    feve_our = np.zeros(nmouse)\n",
    "\n",
    "    for mouse_id in range(nmouse):\n",
    "        pool = False\n",
    "        # load images\n",
    "        if mouse_id == 5:\n",
    "            xrange_max = 176\n",
    "        else:\n",
    "            xrange_max = 130 \n",
    "        img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id], downsample=2)\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "\n",
    "        # load neurons\n",
    "        fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "        spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "        n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "        # split train and validation set\n",
    "        itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "\n",
    "        # normalize spks\n",
    "        spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "\n",
    "        ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "        spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "        spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "        img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "        input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "        seed = 1\n",
    "        nlayers = 1\n",
    "        nconv1 = 64\n",
    "        nconv2 = 64\n",
    "\n",
    "        suffix = ''\n",
    "        if mouse_id == 5: suffix = f'xrange_{xrange_max}'\n",
    "        if suffix != '': suffix += '_'\n",
    "        suffix += f'downsample_2_ks_9'\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx, kernel_size=[9,7])\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "        model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "        # print('test_pred: ', test_pred.shape, test_pred.min(), test_pred.max())\n",
    "\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "        threshold = 0.15\n",
    "        print(f'filtering neurons with FEV > {threshold}')\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "        print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "        feve_ds[mouse_id] = np.mean(test_feve[test_fev > threshold])\n",
    "\n",
    "\n",
    "        # load our 1 layer 192 model\n",
    "        img = data.load_images(data_path, file=os.path.join(data_path, data.img_file_name[mouse_id]), xrange=[xrange_max-130, xrange_max], downsample=1)\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "        img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "        input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "        nconv1 = 192\n",
    "        nconv2 = 192\n",
    "        pool = True\n",
    "\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx)\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, pool=pool)\n",
    "\n",
    "        model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "        # print('test_pred: ', test_pred.shape, test_pred.min(), test_pred.max())\n",
    "\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "        threshold = 0.15\n",
    "        print(f'filtering neurons with FEV > {threshold}')\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "        print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "        feve_our[mouse_id] = np.mean(test_feve[test_fev > threshold])\n",
    "    np.savez(res_path, feve_ds=feve_ds, feve_our=feve_our)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['same_core_1layer_feve_ds'] = feve_ds\n",
    "data_dict['same_core_1layer_feve_our'] = feve_our"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "feve_depth_all = np.zeros((nmouse, 4))\n",
    "feve_LN_all = np.zeros(nmouse)\n",
    "for mouse_id in range(6):\n",
    "    # dat = np.load(f'fullmodel_{data.mouse_names[mouse_id]}_results.npz', allow_pickle=True)\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    fev = dat['fev']\n",
    "    feve_depth = dat['feve_depth']\n",
    "    feve_depth_all[mouse_id] = feve_depth[:, fev>0.15].mean(axis=1)\n",
    "    feve_LN_all[mouse_id] = dat['LNmodel_feve_all'][fev>0.15].mean()\n",
    "data_dict['feve_our_model'] = feve_depth_all\n",
    "data_dict['feve_LN_model'] = feve_LN_all\n",
    "\n",
    "# run Lurz_model_train_test.ipynb first\n",
    "lurz_feve_all = np.load(os.path.join(result_path, 'lurz_feve_all.npy'))\n",
    "data_dict['feve_lurz_model'] = lurz_feve_all\n",
    "print(data_dict['feve_lurz_model'].mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "# res_path = 'outputs/feve_same_core_2layer.npz'\n",
    "res_path = os.path.join(result_path, 'feve_same_core_2layer.npz')\n",
    "if os.path.exists(res_path):\n",
    "    dat = np.load(res_path)\n",
    "    feve_ds = dat['feve_ds']\n",
    "    feve_our = dat['feve_our']\n",
    "else:\n",
    "    feve_ds = np.zeros(nmouse)\n",
    "    feve_our = np.zeros(nmouse)\n",
    "    pool = False\n",
    "    for mouse_id in range(nmouse):\n",
    "        pool = False\n",
    "        # load images\n",
    "        if mouse_id == 5:\n",
    "            xrange_max = 176\n",
    "        else:\n",
    "            xrange_max = 130 \n",
    "        img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id], downsample=2)\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "\n",
    "        # load neurons\n",
    "        fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "        spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "        n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "        # split train and validation set\n",
    "        itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "\n",
    "        # normalize spks\n",
    "        spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "\n",
    "        ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "        spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "        spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "        img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "        input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "        seed = 1\n",
    "        nlayers = 2\n",
    "        nconv1 = 64\n",
    "        nconv2 = 64\n",
    "\n",
    "        suffix = ''\n",
    "        if suffix != '': suffix += '_'\n",
    "        suffix += f'downsample_2_ks_9_7'\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx, kernel_size=[9,7])\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "        model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "        threshold = 0.15\n",
    "        print(f'filtering neurons with FEV > {threshold}')\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "        print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "        feve_ds[mouse_id] = np.mean(test_feve[test_fev > threshold])\n",
    "\n",
    "        img = data.load_images(data_path, file=os.path.join(data_path, data.img_file_name[mouse_id]), xrange=[xrange_max-130, xrange_max])\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "        img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "        input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "        nlayers = 2\n",
    "        nconv1 = 192\n",
    "        nconv2 = 192\n",
    "        pool = True\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx)\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, pool=pool)\n",
    "\n",
    "        model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "        threshold = 0.15\n",
    "        print(f'filtering neurons with FEV > {threshold}')\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "        print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "        feve_our[mouse_id] = np.mean(test_feve[test_fev > threshold])\n",
    "    np.savez(res_path, feve_ds=feve_ds, feve_our=feve_our)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['same_core_2layer_feve_ds'] = feve_ds\n",
    "data_dict['same_core_2layer_feve_our'] = feve_our"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vary kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "res_path = os.path.join(result_path, 'feve_same_core_conv_ks.npz')\n",
    "conv1_ks_list = [7,13,17,21,25,29]\n",
    "conv2_ks_list = [5,7,9,11,13,15]\n",
    "if os.path.exists(res_path):\n",
    "    dat = np.load(res_path)\n",
    "    feve_conv_ks_all = dat['feve_conv_ks_all']\n",
    "else:\n",
    "    pool = True\n",
    "    mouse_id = 3\n",
    "    conv1_ks_list = [7,13,17,21,25,29]\n",
    "    conv2_ks_list = [5,7,9,11,13,15]\n",
    "    feve_conv_ks_all = np.zeros((nmouse, len(conv1_ks_list), len(conv2_ks_list)))\n",
    "    conv1_all = np.zeros((nmouse, 16, 9, 9))\n",
    "    for mouse_id in range(nmouse):\n",
    "        # load images\n",
    "        if mouse_id == 5:\n",
    "            xrange_max = 176\n",
    "        else:\n",
    "            xrange_max = 130 \n",
    "        img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id], downsample=1)\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "\n",
    "        # load neurons\n",
    "        fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "        spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "        n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "        # split train and validation set\n",
    "        itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "\n",
    "        # normalize spks\n",
    "        spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "\n",
    "        ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "        spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "        spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "        img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "        input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "        seed = 1\n",
    "        nlayers = 2\n",
    "        nconv1 = 16\n",
    "        nconv2 = 320\n",
    "    \n",
    "        for conv1_ks in conv1_ks_list:\n",
    "            for conv2_ks in conv2_ks_list:\n",
    "                suffix = ''\n",
    "                if (conv1_ks != 25) or (conv2_ks != 9):\n",
    "                    if suffix != '': suffix += '_'\n",
    "                    suffix += f'ks_{conv1_ks}_{conv2_ks}'\n",
    "                model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx, kernel_size=[conv1_ks, conv2_ks])\n",
    "                model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "                model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "                print('model path: ', model_path)\n",
    "                model = model.to(device)\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                print('loaded model', model_path)\n",
    "\n",
    "                # test model\n",
    "                test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "                test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "                print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "                threshold = 0.15\n",
    "                print(f'filtering neurons with FEV > {threshold}')\n",
    "                valid_idxes = np.where(test_fev > threshold)[0]\n",
    "                print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "                print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "                feve_conv_ks_all[mouse_id, conv1_ks_list.index(conv1_ks), conv2_ks_list.index(conv2_ks)] = np.mean(test_feve[test_fev > threshold])\n",
    "    np.savez(res_path, feve_conv_ks_all=feve_conv_ks_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['same_core_conv_ks_feve'] = feve_conv_ks_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import sup_figure\n",
    "from fig_utils import *\n",
    "root = './outputs'\n",
    "sup_figure.figure_same_core(data_dict, root)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 6: Wx and Wy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mouse\n",
    "# Example neuron Trial 1 vs trial 2\n",
    "mouse_id = 0\n",
    "Wx_all = []\n",
    "Wy_all = []\n",
    "fev_all = []\n",
    "for mouse_id in range(6):\n",
    "    # spks, spks_rep, itrain, ival, fev, istim_test = load_activity(file_path=data.mouse_file_paths[mouse_id], mouse_id=mouse_id)\n",
    "    n_neurons = data.NNs[mouse_id]\n",
    "    ineur = np.arange(0, n_neurons) #np.arange(0, n_neurons, 5)\n",
    "    input_Ly, input_Lx = 66, 130\n",
    "    nlayers = 2\n",
    "    nconv1 = 192\n",
    "    nconv2 = 192\n",
    "\n",
    "    suffix = ''\n",
    "    if mouse_id == 5: suffix += f'xrange_176'\n",
    "    model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, depth_separable=depth_separable)\n",
    "    model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, clamp=clamp, suffix=suffix)\n",
    "\n",
    "    weight_path = os.path.join(weight_path, 'fullmodel', data.mouse_names[mouse_id])\n",
    "    if not os.path.exists(weight_path):\n",
    "        os.makedirs(weight_path)\n",
    "    model_path = os.path.join(weight_path, model_name)\n",
    "    print('model path: ', model_path)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('loaded model', model_path)\n",
    "\n",
    "    Wc = model.readout.Wc.detach().cpu().numpy().squeeze()\n",
    "    # # change model Wx and Wy\n",
    "    Wx = model.readout.Wx.detach().cpu().numpy()\n",
    "    Wy = model.readout.Wy.detach().cpu().numpy()\n",
    "    # outer product of Wx and Wy\n",
    "    Wxy = np.einsum('icj,ick->ijk', Wy, Wx)\n",
    "\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    fev = dat['fev']\n",
    "    valid_idx = np.where(fev > 0.15)[0]\n",
    "    Wx_all.append(Wx[valid_idx])\n",
    "    Wy_all.append(Wy[valid_idx])\n",
    "    fev_all.append(fev[valid_idx])\n",
    "\n",
    "data_dict['fev_all'] = fev_all\n",
    "data_dict['Wx_all'] = Wx_all\n",
    "data_dict['Wy_all'] = Wy_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimodel.utils import weight_bandwidth\n",
    "from scipy.interpolate import interp1d\n",
    "fev_all = data_dict['fev_all']\n",
    "Wx_all = data_dict['Wx_all']\n",
    "Wy_all = data_dict['Wy_all']\n",
    "\n",
    "fev = np.hstack(fev_all)\n",
    "Wx = np.vstack(Wx_all)\n",
    "Wy = np.vstack(Wy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Wx.shape[0]\n",
    "bandwidth_Wx = np.zeros(NN)\n",
    "bandwidth_Wy = np.zeros(NN)\n",
    "centerpos_Wx = np.zeros(NN)\n",
    "centerpos_Wy = np.zeros(NN)\n",
    "for i in range(NN):\n",
    "    bandwidth_Wx[i], centerpos_Wx[i] = weight_bandwidth(Wx[i, 0, :], return_peak=True)\n",
    "    bandwidth_Wy[i], centerpos_Wy[i] = weight_bandwidth(Wy[i, 0, :], return_peak=True)\n",
    "\n",
    "Lx, Ly = Wx.shape[-1], Wy.shape[-1]\n",
    "xmid, ymid = int(Lx/2), int(Ly/2)\n",
    "x_xrange, y_xrange = np.arange(Lx), np.arange(Ly)\n",
    "\n",
    "ineurons = np.arange(0, NN)\n",
    "# Define a common x range for interpolation, e.g., based on the min and max of x_xrange and center positions\n",
    "common_x = np.linspace(np.min(x_xrange - np.max(centerpos_Wx)), \n",
    "                    np.max(x_xrange - np.min(centerpos_Wx)), \n",
    "                    num=len(x_xrange))\n",
    "\n",
    "# Container for interpolated Wx values\n",
    "interp_Wx_values = []\n",
    "\n",
    "for i in ineurons:\n",
    "    # Original x values for this neuron, shifted by its center position\n",
    "    original_x = x_xrange - centerpos_Wx[i]\n",
    "    # Interpolation function for the current neuron's Wx values\n",
    "    interp_func = interp1d(original_x, Wx[i, 0, :], kind='linear', bounds_error=False, fill_value=np.NaN)\n",
    "    # Interpolate onto the common x range and store the result\n",
    "    interp_Wx_values.append(interp_func(common_x))\n",
    "    # plt.plot(original_x, Wx[i, 0, :], color='whitesmoke', alpha=1/255)\n",
    "\n",
    "\n",
    "data_dict['interp_Wx_values'] = interp_Wx_values\n",
    "data_dict['common_x'] = common_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Assumptions and initial setup (Ensure these variables are defined correctly in your context)\n",
    "ineurons = np.arange(0, NN)\n",
    "# Define a common x range for interpolation, e.g., based on the min and max of x_xrange and center positions\n",
    "common_x = np.linspace(np.min(y_xrange - np.max(centerpos_Wy)), \n",
    "                       np.max(y_xrange - np.min(centerpos_Wy)), \n",
    "                       num=len(y_xrange))\n",
    "\n",
    "# Container for interpolated Wx values\n",
    "interp_Wy_values = []\n",
    "\n",
    "for i in ineurons:\n",
    "    # Original x values for this neuron, shifted by its center position\n",
    "    original_x = y_xrange - centerpos_Wy[i]\n",
    "    # Interpolation function for the current neuron's Wx values\n",
    "    interp_func = interp1d(original_x, Wy[i, 0, :], kind='linear', bounds_error=False, fill_value=np.NaN)\n",
    "    # Interpolate onto the common x range and store the result\n",
    "    interp_Wy_values.append(interp_func(common_x))\n",
    "    # plt.plot(original_x, Wy[i, 0, :], color='gray', alpha=0.1)\n",
    "\n",
    "# Calculate the mean of the interpolated Wx values\n",
    "data_dict['interp_Wy_values'] = interp_Wy_values\n",
    "data_dict['common_y'] = common_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey\n",
    "Lx, Ly = 80, 80\n",
    "nconv1 = 192\n",
    "nconv2 = 192\n",
    "use_sensorium_normalization = True\n",
    "model, in_channels = model_builder.build_model(NN=166, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, input_Lx=Lx, input_Ly=Ly)\n",
    "suffix = ''\n",
    "model_name = model_builder.create_model_name('monkeyV1', '2019', n_layers=nlayers, in_channels=in_channels, suffix=suffix, seed=1, use_sensorium_normalization=use_sensorium_normalization)\n",
    "weight_path = os.path.join(weight_path, 'fullmodel', 'monkeyV1')\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "print('model path: ', model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)\n",
    "model = model.to(device)\n",
    "\n",
    "Wc = model.readout.Wc.detach().cpu().numpy().squeeze()\n",
    "# # change model Wx and Wy\n",
    "Wx = model.readout.Wx.detach().cpu().numpy()\n",
    "Wy = model.readout.Wy.detach().cpu().numpy()\n",
    "# outer product of Wx and Wy\n",
    "Wxy = np.einsum('icj,ick->ijk', Wy, Wx)\n",
    "print(Wxy.shape, Wc.shape)\n",
    "print(Wc.shape, Wc.min(), Wc.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Wx.shape[0]\n",
    "bandwidth_Wx = np.zeros(NN)\n",
    "bandwidth_Wy = np.zeros(NN)\n",
    "centerpos_Wx = np.zeros(NN)\n",
    "centerpos_Wy = np.zeros(NN)\n",
    "for i in range(NN):\n",
    "    bandwidth_Wx[i], centerpos_Wx[i] = weight_bandwidth(Wx[i, 0, :], return_peak=True)\n",
    "    bandwidth_Wy[i], centerpos_Wy[i] = weight_bandwidth(Wy[i, 0, :], return_peak=True)\n",
    "\n",
    "Lx, Ly = Wx.shape[-1], Wy.shape[-1]\n",
    "xmid, ymid = int(Lx/2), int(Ly/2)\n",
    "x_xrange, y_xrange = np.arange(Lx), np.arange(Ly)\n",
    "\n",
    "ineurons = np.arange(0, NN)\n",
    "# Define a common x range for interpolation, e.g., based on the min and max of x_xrange and center positions\n",
    "common_x = np.linspace(np.min(x_xrange - np.max(centerpos_Wx)), \n",
    "                    np.max(x_xrange - np.min(centerpos_Wx)), \n",
    "                    num=len(x_xrange))\n",
    "\n",
    "# Container for interpolated Wx values\n",
    "interp_Wx_values = []\n",
    "\n",
    "for i in ineurons:\n",
    "    # Original x values for this neuron, shifted by its center position\n",
    "    original_x = x_xrange - centerpos_Wx[i]\n",
    "    # Interpolation function for the current neuron's Wx values\n",
    "    interp_func = interp1d(original_x, Wx[i, 0, :], kind='linear', bounds_error=False, fill_value=np.NaN)\n",
    "    # Interpolate onto the common x range and store the result\n",
    "    interp_Wx_values.append(interp_func(common_x))\n",
    "    # plt.plot(original_x, Wx[i, 0, :], color='whitesmoke', alpha=1/255)\n",
    "\n",
    "\n",
    "data_dict['monkey_interp_Wx_values'] = interp_Wx_values\n",
    "data_dict['monkey_common_x'] = common_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Assumptions and initial setup (Ensure these variables are defined correctly in your context)\n",
    "ineurons = np.arange(0, NN)\n",
    "# Define a common x range for interpolation, e.g., based on the min and max of x_xrange and center positions\n",
    "common_x = np.linspace(np.min(y_xrange - np.max(centerpos_Wy)), \n",
    "                       np.max(y_xrange - np.min(centerpos_Wy)), \n",
    "                       num=len(y_xrange))\n",
    "\n",
    "# Container for interpolated Wx values\n",
    "interp_Wy_values = []\n",
    "\n",
    "for i in ineurons:\n",
    "    # Original x values for this neuron, shifted by its center position\n",
    "    original_x = y_xrange - centerpos_Wy[i]\n",
    "    # Interpolation function for the current neuron's Wx values\n",
    "    interp_func = interp1d(original_x, Wy[i, 0, :], kind='linear', bounds_error=False, fill_value=np.NaN)\n",
    "    # Interpolate onto the common x range and store the result\n",
    "    interp_Wy_values.append(interp_func(common_x))\n",
    "    # plt.plot(original_x, Wy[i, 0, :], color='gray', alpha=0.1)\n",
    "\n",
    "# Calculate the mean of the interpolated Wx values\n",
    "data_dict['monkey_interp_Wy_values'] = interp_Wy_values\n",
    "data_dict['monkey_common_y'] = common_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import sup_figure\n",
    "from fig_utils import *\n",
    "save_path = './outputs'\n",
    "sup_figure.figure3(data_dict, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 7: reuse conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "NN = 100\n",
    "feve_matrix = np.zeros((nmouse, nmouse, NN))\n",
    "\n",
    "nconv1 = 16\n",
    "nconv2 = 64\n",
    "wc_coef = 0.2\n",
    "hs_readout = 0.03\n",
    "nlayers = 2\n",
    "\n",
    "for mouse_id in range(nmouse):\n",
    "    img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id])\n",
    "    nimg, Ly, Lx = img.shape\n",
    "    print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "    fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "\n",
    "    spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "    n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "    # normalize spks\n",
    "    itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "    spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "    img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "    input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "    ineurons = np.arange(data.NNs_valid[mouse_id])\n",
    "    np.random.seed(42)\n",
    "    ineurons = np.random.choice(ineurons, 100, replace=False)\n",
    "\n",
    "    fev_test = metrics.fev(spks_rep_all)\n",
    "    isort_neurons = np.argsort(fev_test)[::-1]\n",
    "    for mouse_id_base in range(nmouse):\n",
    "        for i, ineuron in enumerate(ineurons):\n",
    "            ineur = [isort_neurons[ineuron]]\n",
    "            spks_rep = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "            if mouse_id_base != mouse_id:\n",
    "                suffix = f'pretrainconv1_{data.mouse_names[mouse_id_base]}_{data.exp_date[mouse_id_base]}'\n",
    "                if mouse_id_base == 5: \n",
    "                    suffix = f'pretrainconv1_{data.mouse_names[mouse_id_base]}_{data.exp_date[mouse_id_base]}_xrange_176'\n",
    "            else: \n",
    "                suffix = ''\n",
    "            model, in_channels = model_builder.build_model(NN=1, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, Wc_coef=wc_coef)\n",
    "            model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], ineuron=ineur[0], n_layers=nlayers, in_channels=in_channels, hs_readout=hs_readout, suffix=suffix)\n",
    "\n",
    "            weight_path = os.path.join(weight_path, 'minimodel', data.mouse_names[mouse_id])\n",
    "            model_path = os.path.join(weight_path, model_name)\n",
    "            print('model path: ', model_path)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print('loaded model', model_path)\n",
    "            model = model.to(device)\n",
    "\n",
    "            # test model\n",
    "            test_pred = model_trainer.test_epoch(model, img_test)\n",
    "            test_fev, test_feve = metrics.feve(spks_rep, test_pred)\n",
    "            print('FEVE (test): ', test_feve)\n",
    "\n",
    "            feve_matrix[mouse_id, mouse_id_base, i] = np.mean(test_feve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['reuse_conv1_feve'] = feve_matrix.transpose(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1 kernels\n",
    "mouse_id = 0\n",
    "conv1_all = []\n",
    "for mouse_id in range(6):\n",
    "    dat = np.load(os.path.join(result_path, f'fullmodel_{data.mouse_names[mouse_id]}_results.npz'), allow_pickle=True)\n",
    "    conv1 = dat['fullmodel_conv1_W']\n",
    "    conv1_all.append(conv1)\n",
    "data_dict['conv1_W'] = np.stack(conv1_all)\n",
    "print(data_dict['conv1_W'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sup_figure\n",
    "from fig_utils import *\n",
    "save_path = './outputs'\n",
    "sup_figure.figure6(data_dict, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 8: pool/no pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = os.path.join(result_path, 'feve_pool_no_pool_mouse.npz')\n",
    "if os.path.exists(res_path):\n",
    "    dat = np.load(res_path)\n",
    "    feve_no_pool = dat['feve_no_pool']\n",
    "    feve_pool = dat['feve_pool']\n",
    "    conv1_no_pool = dat['conv1_no_pool']\n",
    "    conv1_pool = dat['conv1_pool']\n",
    "    feve_small_ks = dat['feve_small_ks']\n",
    "    conv1_small_ks = dat['conv1_small_ks']\n",
    "else:\n",
    "    nmouse = 6\n",
    "    feve_no_pool = np.zeros(nmouse)\n",
    "    feve_pool = np.zeros(nmouse)\n",
    "    conv1_no_pool = np.zeros((nmouse, 16, 25, 25))\n",
    "    conv1_pool = np.zeros((nmouse, 16, 25, 25))\n",
    "    feve_small_ks = np.zeros(nmouse)\n",
    "    conv1_small_ks = np.zeros((nmouse, 16, 9, 9))\n",
    "\n",
    "    for mouse_id in range(nmouse):\n",
    "        # load images\n",
    "        img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id])\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "\n",
    "        # load neurons\n",
    "        fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "        spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "        n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "        # split train and validation set\n",
    "        itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "\n",
    "        # normalize spks\n",
    "        spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "\n",
    "        ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "        spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "        spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "        img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "        input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "        seed = 1\n",
    "        nlayers = 2\n",
    "        nconv1 = 16\n",
    "        nconv2 = 320\n",
    "        pool = False\n",
    "\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx)\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "        model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "        threshold = 0.15\n",
    "        print(f'filtering neurons with FEV > {threshold}')\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "        print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "        feve_no_pool[mouse_id] = np.mean(test_feve[test_fev > threshold])\n",
    "\n",
    "        conv1_no_pool[mouse_id] = model.core.features.layer0.conv.weight.cpu().detach().numpy().squeeze()\n",
    "\n",
    "        pool = True\n",
    "\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx)\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, pool=pool)\n",
    "\n",
    "        model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "        threshold = 0.15\n",
    "        print(f'filtering neurons with FEV > {threshold}')\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "        print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "        feve_pool[mouse_id] = np.mean(test_feve[test_fev > threshold])\n",
    "        conv1_pool[mouse_id] = model.core.features.layer0.conv.weight.cpu().detach().numpy().squeeze()\n",
    "\n",
    "        conv1_ks = 9\n",
    "        conv2_ks = 9\n",
    "\n",
    "        suffix = ''\n",
    "        if (conv1_ks != 25) or (conv2_ks != 9):\n",
    "            if suffix != '': suffix += '_'\n",
    "            suffix += f'ks_{conv1_ks}_{conv2_ks}'\n",
    "        model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx, kernel_size=[conv1_ks, conv2_ks])\n",
    "        model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "        model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "        print('model path: ', model_path)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('loaded model', model_path)\n",
    "\n",
    "        # test model\n",
    "        test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "        test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "        print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "        threshold = 0.15\n",
    "        print(f'filtering neurons with FEV > {threshold}')\n",
    "        valid_idxes = np.where(test_fev > threshold)[0]\n",
    "        print(f'valid neurons: {len(valid_idxes)} / {len(test_fev)}')\n",
    "        print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "        feve_small_ks[mouse_id] = np.mean(test_feve[test_fev > threshold])\n",
    "        conv1_small_ks[mouse_id] = model.core.features.layer0.conv.weight.cpu().detach().numpy().squeeze()\n",
    "\n",
    "    np.savez(res_path, feve_no_pool=feve_no_pool, feve_pool=feve_pool, conv1_no_pool=conv1_no_pool, conv1_pool=conv1_pool, feve_small_ks=feve_small_ks, conv1_small_ks=conv1_small_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['feve_no_pool'] = feve_no_pool\n",
    "data_dict['feve_pool'] = feve_pool\n",
    "data_dict['conv1_no_pool'] = conv1_no_pool\n",
    "data_dict['conv1_pool'] = conv1_pool\n",
    "data_dict['feve_small_ks'] = feve_small_ks\n",
    "data_dict['conv1_small_ks'] = conv1_small_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import sup_figure\n",
    "root = './outputs'\n",
    "sup_figure.figure_pool_nopool(data_dict, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 9. sparsity penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mouse validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_id = 0\n",
    "hs_list = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.5]\n",
    "nhs = len(hs_list)\n",
    "feve_all = []\n",
    "nconv2_all = []\n",
    "\n",
    "dat = np.load(os.path.join(result_path, f'{data.mouse_names[mouse_id]}_{data.exp_date[mouse_id]}_minimodel_16_64_choose_param_val_result.npz'))\n",
    "feve_all = dat['feve_val']\n",
    "nconv2_all = dat['nconv2']\n",
    "print(feve_all.shape, nconv2_all.shape)\n",
    "\n",
    "data_dict['mouse_feve_val'] = feve_all\n",
    "data_dict['mouse_nconv2_val'] = nconv2_all\n",
    "\n",
    "dat = np.load(os.path.join(result_path, f'{data.mouse_names[mouse_id]}_{data.exp_date[mouse_id]}_minimodel_16_64_choose_param_5k_val_result.npz'))\n",
    "feve_all = dat['feve_val']\n",
    "nconv2_all = dat['nconv2']\n",
    "print(feve_all.shape, nconv2_all.shape)\n",
    "\n",
    "data_dict['mouse_feve_val_5k'] = feve_all\n",
    "data_dict['mouse_nconv2_val_5k'] = nconv2_all\n",
    "\n",
    "# monkey\n",
    "dat = np.load(os.path.join(result_path, \"minimodel_monkey_result.npz\"))\n",
    "feve_all = dat['param_search_feve_val']\n",
    "nconv2_all = dat['param_search_nconv2_val']\n",
    "print(feve_all.shape, nconv2_all.shape)\n",
    "\n",
    "data_dict['monkey_feve_val'] = feve_all\n",
    "data_dict['monkey_nconv2_val'] = nconv2_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mouse test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "hs_list = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.5]\n",
    "nhs = len(hs_list)\n",
    "feve_all = []\n",
    "nconv2_all = []\n",
    "for mouse_id in range(nmouse):\n",
    "    dat = np.load(os.path.join(result_path, f'{data.mouse_names[mouse_id]}_{data.exp_date[mouse_id]}_minimodel_16_64_choose_param_result.npz'))\n",
    "    feve = dat['feve_all']\n",
    "    wc = dat['wc_all']\n",
    "    nconv2 = np.sum(np.abs(wc)>0.01, axis=2)\n",
    "    feve_all.append(feve)\n",
    "    nconv2_all.append(nconv2)\n",
    "feve_all = np.vstack(feve_all)\n",
    "nconv2_all = np.vstack(nconv2_all)\n",
    "print(feve_all.shape, nconv2_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['sparsity_feve_all'] = feve_all\n",
    "data_dict['sparsity_nconv2_all'] = nconv2_all\n",
    "data_dict['sparsity_hs'] = hs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mouse 5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse = 6\n",
    "hs_list = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.5]\n",
    "nhs = len(hs_list)\n",
    "feve_all = []\n",
    "nconv2_all = []\n",
    "for mouse_id in range(nmouse):\n",
    "    dat = np.load(os.path.join(result_path, f'{data.mouse_names[mouse_id]}_{data.exp_date[mouse_id]}_minimodel_16_64_choose_param_5k_result.npz'))\n",
    "    feve = dat['feve_all']\n",
    "    wc = dat['wc_all']\n",
    "    nconv2 = np.sum(np.abs(wc)>0.01, axis=2)\n",
    "    feve_all.append(feve)\n",
    "    nconv2_all.append(nconv2)\n",
    "feve_all = np.vstack(feve_all)\n",
    "nconv2_all = np.vstack(nconv2_all)\n",
    "print(feve_all.shape, nconv2_all.shape)\n",
    "\n",
    "data_dict['sparsity_feve_5k_all'] = feve_all\n",
    "data_dict['sparsity_nconv2_5k_all'] = nconv2_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat = np.load('./outputs/minimodel_monkey_result.npz')\n",
    "dat = np.load(os.path.join(result_path, 'minimodel_monkey_result.npz'))\n",
    "feve_hs_all = dat['feve_hs_all']\n",
    "wc_hs_all = dat['wc_hs_all']\n",
    "nconv2_all = np.sum(np.abs(wc_hs_all)>0.01, axis=2)\n",
    "hs_list = dat['hs_list']\n",
    "data_dict['sparsity_monkey_feve_all'] = feve_hs_all \n",
    "data_dict['sparsity_monkey_nconv2_all'] = nconv2_all\n",
    "data_dict['sparsity_monkey_hs'] = hs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fullmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(result_path, f'fullmodel_hoyer_loss_result_{data.mouse_names[mouse_id]}_wc.npz')\n",
    "hs_list = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.5]\n",
    "\n",
    "if os.path.exists(fpath):\n",
    "    dat = np.load(fpath)\n",
    "    feve_all = dat['feve_all']\n",
    "    wc_all = dat['wc_all']\n",
    "    n_wc_all = dat['n_wc_all']\n",
    "    # hs_list = dat['hs_list']\n",
    "    data_dict['fullmodel_hoyer_feve_all'] = feve_all\n",
    "    data_dict['fullmodel_hoyer_wc_all'] = wc_all\n",
    "    data_dict['fullmodel_hoyer_n_wc_all'] = n_wc_all\n",
    "    data_dict['fullmodel_hoyer_hs'] = hs_list\n",
    "else:\n",
    "    nmouse = 6\n",
    "    pool = True\n",
    "    mouse_id = 5\n",
    "    feve_all_mice = []\n",
    "    wc_all_mice = []\n",
    "    n_wc_all_mice = []\n",
    "    for mouse_id in range(nmouse):\n",
    "        if mouse_id == 5:\n",
    "            xrange_max = 176\n",
    "        else:\n",
    "            xrange_max = 130 \n",
    "        img = data.load_images(data_path, file=data.img_file_name[mouse_id])\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "\n",
    "        # load neurons\n",
    "        fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "        spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "        n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "        # split train and validation set\n",
    "        itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "\n",
    "        # normalize spks\n",
    "        spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "\n",
    "        ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "        # spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "        spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "        # img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "        input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "        seed = 1\n",
    "        nlayers = 2\n",
    "        nconv1 = 16\n",
    "        nconv2 = 320\n",
    "        conv1_ks = 25\n",
    "        conv2_ks = 9\n",
    "        feve_all = np.zeros(len(hs_list))\n",
    "        wc_all = []\n",
    "        ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "        for i in range(len(hs_list)): \n",
    "            hs_readout = hs_list[i]\n",
    "            suffix = ''\n",
    "            if mouse_id == 5: suffix = f'xrange_{xrange_max}'\n",
    "            if (conv1_ks != 25) or (conv2_ks != 9):\n",
    "                if suffix != '': suffix += '_'\n",
    "                suffix += f'ks_{conv1_ks}_{conv2_ks}'\n",
    "            model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, depth_separable=depth_separable, input_Ly=input_Ly, input_Lx=input_Lx, kernel_size=[conv1_ks, conv2_ks])\n",
    "            model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, clamp=clamp, seed=seed, suffix=suffix, pool=pool, hs_readout=hs_readout)\n",
    "\n",
    "            model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "            print('model path: ', model_path)\n",
    "            model = model.to(device)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print('loaded model', model_path)\n",
    "\n",
    "            # test model\n",
    "            test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "            test_fev, test_feve = metrics.feve(spks_rep_all, test_pred)\n",
    "            print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "            threshold = 0.15\n",
    "            print(f'filtering neurons with FEV > {threshold}')\n",
    "            valid_idxes_tmp = np.where(test_fev > threshold)[0]\n",
    "            print(f'valid neurons: {len(valid_idxes_tmp)} / {len(test_fev)}')\n",
    "            print(f'FEVE (test, FEV>0.15): {np.mean(test_feve[test_fev > threshold])}')\n",
    "            feve_all[i] = np.mean(test_feve[test_fev > threshold])\n",
    "            wc_all.append(model.readout.Wc.cpu().detach().numpy().squeeze())\n",
    "\n",
    "        valid_idxes = np.where(test_fev > 0.15)[0]\n",
    "        n_wc_all = []\n",
    "        valid_wc_all = []\n",
    "        for wc in wc_all:\n",
    "            wc = wc[valid_idxes]\n",
    "            n_wc = np.sum(np.abs(wc) > 0.01, axis=1)\n",
    "            n_wc_all.append(np.mean(n_wc))\n",
    "            valid_wc_all.append(wc)\n",
    "        feve_all_mice.append(feve_all) # (nmouse, nhs)\n",
    "        wc_all_mice.append(valid_wc_all) # (nmouse, nhs, n_neurons, 320)\n",
    "        n_wc_all_mice.append(n_wc_all) # (nmouse, nhs)\n",
    "\n",
    "    feve_all_mice = np.array(feve_all_mice)\n",
    "    for i, wc_all in enumerate(wc_all_mice):\n",
    "        wc_all_mice[i] = np.array(wc_all)\n",
    "    n_wc_all_mice = np.array(n_wc_all_mice)\n",
    "    wc_all_mice = np.array(wc_all_mice)\n",
    "    np.savez(fpath, feve_all=feve_all_mice, wc_all=wc_all_mice[3], n_wc_all=n_wc_all_mice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import sup_figure\n",
    "root = './outputs'\n",
    "sup_figure.figure4(data_dict, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 10: 5k vs 30k FEVE with varying #neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all neurons FEVE\n",
    "n_neuron = 1\n",
    "nmouse = 6\n",
    "neuron_numbers = np.geomspace(1, 1000, num=10, dtype=int)\n",
    "neuron_numbers = np.unique(np.concatenate(([1], neuron_numbers)))  # Ensure 1 is included and remove duplicates\n",
    "seed_numbers = np.linspace(20, 1, num=len(neuron_numbers), dtype=int)\n",
    "feves_all = np.zeros((2, 2, nmouse, seed_numbers[0], n_neuron))\n",
    "feves_pretrain_all = np.zeros((2, 2, nmouse, seed_numbers[0], n_neuron))\n",
    "nstim_list = [5000, 30000]\n",
    "\n",
    "res_path = os.path.join(result_path, 'feve_nstim_pretrain_results.npz')\n",
    "if os.path.exists(res_path):\n",
    "    dat = np.load(res_path)\n",
    "    feves_all = dat['feves_all']\n",
    "    feves_pretrain_all = dat['feves_pretrain_all']\n",
    "else:\n",
    "    pool = True\n",
    "    mouse_id = 0\n",
    "\n",
    "    for mouse_id in range(nmouse):\n",
    "        # load images\n",
    "        img = data.load_images(data_path, mouse_id, file=data.img_file_name[mouse_id])\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "\n",
    "        # load neurons\n",
    "        fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "        spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "        n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "        # split train and validation set\n",
    "        itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "\n",
    "        # normalize spks\n",
    "        spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "\n",
    "        ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "\n",
    "        spks_val = torch.from_numpy(spks[ival][:,ineur]) \n",
    "        spks_rep_all = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "        img_val = torch.from_numpy(img[istim_train][ival]).to(device).unsqueeze(1)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "\n",
    "        for m, nstims in enumerate(nstim_list):\n",
    "            if len(itrain) < nstims:\n",
    "                print('not enough training stimuli, using all stimuli')\n",
    "                nstims = len(itrain)\n",
    "\n",
    "            input_Ly, input_Lx = img_test.shape[-2:]\n",
    "            seed = 1\n",
    "            nlayers = 2\n",
    "            nconv1 = 16\n",
    "            nconv2 = 320\n",
    "            conv1_ks = 25\n",
    "            conv2_ks = 9\n",
    "            suffix = ''\n",
    "            suffix += f'nstims_{nstims}'\n",
    "            ineur = np.arange(0, n_max_neurons) #np.arange(0, n_neurons, 5)\n",
    "            model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx, kernel_size=[conv1_ks, conv2_ks])\n",
    "            model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "            model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "            print('model path: ', model_path)\n",
    "            model = model.to(device)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print('loaded model', model_path)\n",
    "\n",
    "            # test model\n",
    "            test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "            test_fev, feve_all = metrics.feve(spks_rep_all, test_pred)\n",
    "            print('FEVE (test, all): ', np.mean(feve_all))\n",
    "\n",
    "            fev_test = metrics.fev(spks_rep_all)\n",
    "            valid_idxes = np.where(fev_test > 0.15)[0]\n",
    "            print(len(valid_idxes), len(fev_test))\n",
    "\n",
    "            # without pretrain\n",
    "            i = np.where(neuron_numbers == n_neuron)[0][0]\n",
    "            for seed in range(1, seed_numbers[i]+1):\n",
    "                if n_neuron >= len(valid_idxes):\n",
    "                    print(f'not enough neurons with FEV > 0.15, using all neurons')\n",
    "                    ineur = valid_idxes.copy()\n",
    "                    n_neuron = len(valid_idxes)\n",
    "                else:\n",
    "                    np.random.seed(n_neuron*seed)\n",
    "                    ineur = np.random.choice(valid_idxes, size=n_neuron, replace=False)\n",
    "                spks_rep_all_tmp = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "                suffix = ''\n",
    "                if n_neuron != -1:\n",
    "                    suffix = f'nneurons_{n_neuron}'\n",
    "                if suffix != '': suffix += '_'\n",
    "                suffix += f'nstims_{nstims}'\n",
    "                model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx, kernel_size=[conv1_ks, conv2_ks])\n",
    "                model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "                model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "                print('model path: ', model_path)\n",
    "                model = model.to(device)\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                print('loaded model', model_path)\n",
    "\n",
    "                # test model\n",
    "                test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "                test_fev, test_feve = metrics.feve(spks_rep_all_tmp, test_pred)\n",
    "                print('FEVE (test, all): ', np.mean(test_feve))\n",
    "                \n",
    "                feves_all[m, 0, mouse_id, seed-1] = test_feve # feve from model trained with n neurons\n",
    "                feves_all[m, 1, mouse_id, seed-1] = feve_all[ineur] # feve from model trained with all neurons\n",
    "\n",
    "            # load pretrained model FEVE\n",
    "            pretrain_mouse_id = mouse_id\n",
    "            feves_pretrain = np.zeros((2, len(seed_numbers), n_neuron))\n",
    "            i = np.where(neuron_numbers == n_neuron)[0][0]\n",
    "            for seed in range(1, seed_numbers[i]+1):\n",
    "                if n_neuron >= len(valid_idxes):\n",
    "                    print(f'not enough neurons with FEV > 0.15, using all neurons')\n",
    "                    ineur = valid_idxes.copy()\n",
    "                    n_neuron = len(valid_idxes)\n",
    "                else:\n",
    "                    np.random.seed(n_neuron*seed)\n",
    "                    ineur = np.random.choice(valid_idxes, size=n_neuron, replace=False)\n",
    "                spks_rep_all_tmp = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "                suffix = ''\n",
    "                if n_neuron != -1:\n",
    "                    suffix = f'nneurons_{n_neuron}'\n",
    "                if suffix != '': suffix += '_'\n",
    "                suffix += f'nstims_{nstims}'\n",
    "                if suffix != '': suffix += '_'\n",
    "                suffix += f'pretrainconv1_{data.mouse_names[pretrain_mouse_id]}_{data.exp_date[pretrain_mouse_id]}'\n",
    "                if nstims == 5000:\n",
    "                    if suffix != '': suffix += '_'\n",
    "                    suffix += f'nstims_{nstims}'\n",
    "                model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx, kernel_size=[conv1_ks, conv2_ks])\n",
    "                model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "                model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "                print('model path: ', model_path)\n",
    "                model = model.to(device)\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                print('loaded model', model_path)\n",
    "\n",
    "                # test model\n",
    "                test_pred = model_trainer.test_epoch(model, img_test)\n",
    "\n",
    "                test_fev, test_feve = metrics.feve(spks_rep_all_tmp, test_pred)\n",
    "                print('FEVE (test, all): ', np.mean(test_feve))\n",
    "\n",
    "                feves_pretrain_all[m, 0, mouse_id, seed-1] = test_feve\n",
    "                feves_pretrain_all[m, 1, mouse_id, seed-1] = feve_all[ineur]\n",
    "    np.savez(res_path, feves_all=feves_all, feves_pretrain_all=feves_pretrain_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['nstim_feve_all'] = feves_all\n",
    "data_dict['nstim_feve_pretrain_all'] = feves_pretrain_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import sup_figure\n",
    "root = './outputs'\n",
    "sup_figure.figure_vary_nneuron(data_dict, root)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 11: model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sup_figure\n",
    "from fig_utils import *\n",
    "save_path = './outputs'\n",
    "sup_figure.figure5(data_dict, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 12: conv2 clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_channel_output(model, img_test, batch_size=100):\n",
    "    model.eval()\n",
    "    n_test = img_test.shape[0]\n",
    "    conv2_features = []\n",
    "    conv2_indepth_features = []\n",
    "    with torch.no_grad():\n",
    "        for k in np.arange(0, n_test, batch_size):\n",
    "            kend = min(k+batch_size, n_test)\n",
    "            img_batch = img_test[k:kend]\n",
    "\n",
    "\n",
    "            x = model.core.features.layer0(img_batch)\n",
    "            conv2_indepth_fv = model.core.features.layer1.ds_conv.in_depth_conv(x)\n",
    "            x = model.core.features.layer1.ds_conv.spatial_conv(conv2_indepth_fv)\n",
    "            x = model.core.features.layer1.norm(x)\n",
    "            conv2_relu_fvs = model.core.features.layer1.activation(x)\n",
    "            # print('after in_depth_conv: ', conv2_indepth_fv.shape, conv2_indepth_fv.max(), conv2_indepth_fv.min())\n",
    "            # print('after conv2_relu: ', conv2_relu_fvs.shape, conv2_relu_fvs.max(), conv2_relu_fvs.min())\n",
    "            conv2_fv = conv2_relu_fvs[:, :, 16, 32]\n",
    "            conv2_indepth_fv = conv2_indepth_fv[:, :, 16, 32]\n",
    "            conv2_features.append(conv2_fv.detach().cpu().numpy())\n",
    "            conv2_indepth_features.append(conv2_indepth_fv.detach().cpu().numpy())\n",
    "            # spks_test_pred[k:kend] = spks_pred\n",
    "    conv2_features = np.vstack(conv2_features)\n",
    "    conv2_indepth_features = np.vstack(conv2_indepth_features)\n",
    "    return conv2_indepth_features, conv2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 600 neurons minimodels\n",
    "# save spatial conv, 1x1 conv, and mouse_id\n",
    "# get conv2 channel responses of the train images\n",
    "res_path = os.path.join(result_path, 'conv2_analysis.npz')\n",
    "if os.path.exists(res_path):\n",
    "    dat = np.load(res_path)\n",
    "    feve_all = dat['feve_all']\n",
    "    in_depth_conv_all = dat['in_depth_conv_all']\n",
    "    spatial_conv_all = dat['spatial_conv_all']\n",
    "    Wc_all = dat['Wc_all']\n",
    "    channel_resp_all = dat['channel_resp_all']\n",
    "    channel_indepth_all = dat['channel_indepth_all']\n",
    "else: \n",
    "    nmouse = 6\n",
    "    NN = 100\n",
    "    feve_all = np.zeros((nmouse, NN))\n",
    "    in_depth_conv_all = np.zeros((nmouse, NN, 64, 16))\n",
    "    spatial_conv_all = np.zeros((nmouse, NN, 64, 9, 9))\n",
    "    Wc_all = np.zeros((nmouse, NN, 64))\n",
    "    nconv1 = 16\n",
    "    nconv2 = 64\n",
    "    wc_coef = 0.2\n",
    "    hs_readout = 0.03\n",
    "    nlayers = 2\n",
    "    xrange_max = 130\n",
    "\n",
    "    # use the same train images for all models\n",
    "    mouse_id = 3\n",
    "    img = data.load_images(data_path, file=os.path.join(data_path, data.img_file_name[mouse_id]), xrange=[xrange_max-130, xrange_max])\n",
    "    nimg, Ly, Lx = img.shape\n",
    "    print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "    img_train = torch.from_numpy(img[500:30000]).to(device).unsqueeze(1) \n",
    "    print('img_train: ', img_train.shape)\n",
    "    ntrain = img_train.shape[0]\n",
    "    channel_resp_all = np.zeros((nmouse, NN, ntrain, 64))\n",
    "    channel_indepth_all = np.zeros((nmouse, NN, ntrain, 64))\n",
    "\n",
    "    for mouse_id in range(nmouse):\n",
    "        if mouse_id == 5: xrange_max = 176\n",
    "        else: xrange_max = 130\n",
    "        img = data.load_images(data_path, file=os.path.join(data_path, data.img_file_name[mouse_id]), xrange=[xrange_max-130, xrange_max])\n",
    "        nimg, Ly, Lx = img.shape\n",
    "        print('img: ', img.shape, img.min(), img.max(), img.dtype)\n",
    "        fname = '%s_nat60k_%s.npz'%(data.db[mouse_id]['mname'], data.db[mouse_id]['datexp'])\n",
    "\n",
    "        spks, istim_train, istim_test, xpos, ypos, spks_rep_all = data.load_neurons(file_path = os.path.join(data_path, fname), mouse_id = mouse_id)\n",
    "        n_stim, n_max_neurons = spks.shape\n",
    "\n",
    "        # normalize spks\n",
    "        itrain, ival = data.split_train_val(istim_train, train_frac=0.9)\n",
    "        spks, spks_rep_all = data.normalize_spks(spks, spks_rep_all, itrain)\n",
    "        img_test = torch.from_numpy(img[istim_test]).to(device).unsqueeze(1)\n",
    "        input_Ly, input_Lx = img_test.shape[-2:]\n",
    "\n",
    "        ineurons = np.arange(data.NNs_valid[mouse_id])\n",
    "        np.random.seed(42)\n",
    "        ineurons = np.random.choice(ineurons, 100, replace=False)\n",
    "\n",
    "        fev_test = metrics.fev(spks_rep_all)\n",
    "        isort_neurons = np.argsort(fev_test)[::-1]\n",
    "        # for mouse_id_base in range(nmouse):\n",
    "        mouse_id_base = 0\n",
    "        for i, ineuron in enumerate(ineurons):\n",
    "            ineur = [isort_neurons[ineuron]]\n",
    "            spks_rep = [spks_rep_all[i][:,ineur] for i in range(len(spks_rep_all))]\n",
    "\n",
    "            if mouse_id_base != mouse_id:\n",
    "                suffix = f'pretrainconv1_{data.mouse_names[mouse_id_base]}_{data.exp_date[mouse_id_base]}'\n",
    "                if mouse_id_base == 5: \n",
    "                    suffix = f'pretrainconv1_{data.mouse_names[mouse_id_base]}_{data.exp_date[mouse_id_base]}_xrange_176'\n",
    "            else: \n",
    "                suffix = ''\n",
    "            model, in_channels = model_builder.build_model(NN=1, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, Wc_coef=wc_coef)\n",
    "            model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], ineuron=ineur[0], n_layers=nlayers, in_channels=in_channels, hs_readout=hs_readout, suffix=suffix)\n",
    "\n",
    "            model_path = os.path.join(weight_path, 'minimodel', model_name)\n",
    "            print('model path: ', model_path)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print('loaded model', model_path)\n",
    "            model = model.to(device)\n",
    "\n",
    "            in_depth_conv_all[mouse_id, i] = model.core.features.layer1.ds_conv.in_depth_conv.weight.cpu().detach().numpy().squeeze()\n",
    "            spatial_conv_all[mouse_id, i] = model.core.features.layer1.ds_conv.spatial_conv.weight.cpu().detach().numpy().squeeze()\n",
    "            Wc_all[mouse_id, i] = model.readout.Wc.cpu().detach().numpy().squeeze()\n",
    "\n",
    "            # test model\n",
    "            test_pred = model_trainer.test_epoch(model, img_test)\n",
    "            test_fev, test_feve = metrics.feve(spks_rep, test_pred)\n",
    "            print('FEVE (test): ', test_feve)\n",
    "            feve_all[mouse_id, i] = np.mean(test_feve)\n",
    "            channel_indepth_all[mouse_id, i], channel_resp_all[mouse_id, i] = test_channel_output(model, img_train)\n",
    "    np.savez(res_path, feve_all=feve_all, in_depth_conv_all=in_depth_conv_all, spatial_conv_all=spatial_conv_all, Wc_all=Wc_all, channel_resp_all=channel_resp_all, channel_indepth_all=channel_indepth_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne of 1x1 conv (color by mouse_id)\n",
    "print(in_depth_conv_all.shape, spatial_conv_all.shape, Wc_all.shape)\n",
    "nmouse, NN, nconv2, nconv1 = in_depth_conv_all.shape\n",
    "in_depth_conv_all_flat = in_depth_conv_all.reshape(nmouse*NN*nconv2, nconv1)\n",
    "# normalize by the max value in each row \n",
    "# in_depth_conv_all_flat = in_depth_conv_all_flat / np.abs(in_depth_conv_all_flat).max(axis=1)[:, None]\n",
    "# normalize but the norm of each row \n",
    "in_depth_conv_all_flat = in_depth_conv_all_flat / np.linalg.norm(in_depth_conv_all_flat, axis=1)[:, None]\n",
    "mouse_ids = np.zeros(Wc_all.shape)\n",
    "for i in range(nmouse):\n",
    "    mouse_ids[i] = i\n",
    "mouse_ids_flat = mouse_ids.flatten()\n",
    "Wc_all_flat = Wc_all.flatten()\n",
    "print(in_depth_conv_all_flat.shape, mouse_ids_flat.shape, Wc_all_flat.shape)\n",
    "\n",
    "# select the valid channels\n",
    "valid_idxes = np.where(np.abs(Wc_all_flat) > 0.01)[0]\n",
    "print('valid channels: ', len(valid_idxes))\n",
    "in_depth_conv_all_flat = in_depth_conv_all_flat[valid_idxes]\n",
    "mouse_ids_flat = mouse_ids_flat[valid_idxes]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_2d = tsne.fit_transform(in_depth_conv_all_flat)\n",
    "print('X_2d: ', X_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['tsne_conv2_1x1'] = X_2d\n",
    "data_dict['mouse_ids_flat'] = mouse_ids_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans clustering of 1x1 conv (color by mouse_id)\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X_2d)\n",
    "cluster_labels = kmeans.labels_\n",
    "print('cluster_labels: ', cluster_labels.shape)\n",
    "\n",
    "# cluster centers from kmeans\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "print('cluster_centers: ', cluster_centers.shape)\n",
    "\n",
    "# find the most representative channel for each cluster\n",
    "from scipy.spatial import distance\n",
    "cluster_center_idxes = []\n",
    "cluster_center_samples = []\n",
    "for i in range(n_clusters):\n",
    "    idxes = np.where(cluster_labels == i)[0]\n",
    "    samples = X_2d[idxes]\n",
    "    dists = distance.cdist([cluster_centers[i]], samples)\n",
    "    center_idx = idxes[np.argmin(dists)]\n",
    "    cluster_center_idxes.append(center_idx)\n",
    "    cluster_center_samples.append(samples[np.argmin(dists)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['cluster_labels'] = cluster_labels\n",
    "data_dict['center_cluster_samples'] = cluster_center_samples\n",
    "data_dict['cluster_center_idxes'] = cluster_center_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load conv1 weights of mouse 0\n",
    "mouse_id = 0\n",
    "seed = 1\n",
    "nlayers = 2\n",
    "nconv1 = 16\n",
    "nconv2 = 320\n",
    "n_max_neurons = data.NNs[mouse_id]\n",
    "ineur = np.arange(0, n_max_neurons) \n",
    "input_Ly, input_Lx = 66, 130\n",
    "model, in_channels = model_builder.build_model(NN=len(ineur), n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2, pool=pool, input_Ly=input_Ly, input_Lx=input_Lx)\n",
    "model_name = model_builder.create_model_name(data.mouse_names[mouse_id], data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels, seed=seed, suffix=suffix, pool=pool)\n",
    "\n",
    "model_path = os.path.join(weight_path, 'fullmodel', model_name)\n",
    "print('model path: ', model_path)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)\n",
    "\n",
    "conv1_W = model.core.features.layer0.conv.weight.cpu().detach().numpy().squeeze()\n",
    "print(f'conv1_W: {conv1_W.shape}')\n",
    "\n",
    "ori_cluster_centers = in_depth_conv_all_flat[cluster_center_idxes]\n",
    "print('ori_cluster_centers: ', ori_cluster_centers.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['conv1_W'] = conv1_W\n",
    "data_dict['ori_cluster_centers'] = ori_cluster_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spatial conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmouse, NN, nconv2, Ly, Lx = spatial_conv_all.shape\n",
    "spatial_conv_all_flat = spatial_conv_all.reshape(nmouse*NN*nconv2, Ly*Lx)\n",
    "spatial_conv_all_flat = spatial_conv_all_flat / np.linalg.norm(spatial_conv_all_flat, axis=1)[:, None]\n",
    "mouse_ids = np.zeros(Wc_all.shape)\n",
    "for i in range(nmouse):\n",
    "    mouse_ids[i] = i\n",
    "mouse_ids_flat = mouse_ids.flatten()\n",
    "Wc_all_flat = Wc_all.flatten()\n",
    "print(spatial_conv_all_flat.shape, mouse_ids_flat.shape, Wc_all_flat.shape)\n",
    "\n",
    "# select the valid channels\n",
    "valid_idxes = np.where(np.abs(Wc_all_flat) > 0.01)[0]\n",
    "print('valid channels: ', len(valid_idxes))\n",
    "spatial_conv_all_flat = spatial_conv_all_flat[valid_idxes]\n",
    "mouse_ids_flat = mouse_ids_flat[valid_idxes]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_2d = tsne.fit_transform(spatial_conv_all_flat)\n",
    "print('X_2d: ', X_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['tsne_conv2_spatial'] = X_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans clustering of 1x1 conv (color by mouse_id)\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X_2d)\n",
    "cluster_labels = kmeans.labels_\n",
    "print('cluster_labels: ', cluster_labels.shape)\n",
    "\n",
    "# cluster centers from kmeans\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "print('cluster_centers: ', cluster_centers.shape)\n",
    "\n",
    "# find the most representative channel for each cluster\n",
    "from scipy.spatial import distance\n",
    "cluster_center_idxes = []\n",
    "cluster_center_samples = []\n",
    "for i in range(n_clusters):\n",
    "    idxes = np.where(cluster_labels == i)[0]\n",
    "    samples = X_2d[idxes]\n",
    "    dists = distance.cdist([cluster_centers[i]], samples)\n",
    "    center_idx = idxes[np.argmin(dists)]\n",
    "    cluster_center_idxes.append(center_idx)\n",
    "    cluster_center_samples.append(samples[np.argmin(dists)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['spatial_cluster_labels'] = cluster_labels\n",
    "data_dict['spatial_center_cluster_samples'] = cluster_center_samples\n",
    "data_dict['spatial_cluster_center_idxes'] = cluster_center_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_cluster_centers = spatial_conv_all_flat[cluster_center_idxes].reshape(len(cluster_center_idxes), Ly, Lx)\n",
    "print('ori_cluster_centers: ', ori_cluster_centers.shape)\n",
    "data_dict['spatial_ori_cluster_centers'] = ori_cluster_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## channel responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(channel_resp_all.shape)\n",
    "print(channel_indepth_all.shape)\n",
    "from numpy import linalg as LA\n",
    "conv2_fv = True\n",
    "if conv2_fv: channel_resp = channel_resp_all # wether use the conv2 features or conv2 indepth features\n",
    "else: channel_resp = channel_indepth_all\n",
    "nmouse, NN, ntrain, nconv2 = channel_resp.shape\n",
    "channel_resp_all_flat = channel_resp.transpose(0,1,3,2).reshape(nmouse*NN*nconv2, ntrain)\n",
    "# channel_resp_all_flat = channel_resp_all_flat / (np.abs(channel_resp_all_flat).max(axis=1)[:, None] + 1e-6)\n",
    "# normalize by the norm of each row\n",
    "channel_resp_all_flat = channel_resp_all_flat / LA.norm(channel_resp_all_flat, axis=1)[:, None]\n",
    "mouse_ids = np.zeros(Wc_all.shape)\n",
    "for i in range(nmouse):\n",
    "    mouse_ids[i] = i    \n",
    "mouse_ids_flat = mouse_ids.flatten()\n",
    "Wc_all_flat = Wc_all.flatten()\n",
    "print(channel_resp_all_flat.shape, mouse_ids_flat.shape, Wc_all_flat.shape)\n",
    "valid_idxes = np.where(np.abs(Wc_all_flat) > 0.01)[0]\n",
    "Wc_all_flat = Wc_all_flat[valid_idxes]\n",
    "channel_resp_all_flat = channel_resp_all_flat[valid_idxes]\n",
    "mouse_ids_flat = mouse_ids_flat[valid_idxes]\n",
    "print(channel_resp_all_flat.shape, mouse_ids_flat.shape, Wc_all_flat.shape)\n",
    "\n",
    "# pca to 200 dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=200)\n",
    "X_pca = pca.fit_transform(channel_resp_all_flat)\n",
    "print('X_pca: ', X_pca.shape)\n",
    "\n",
    "# tsne of conv2 channel responses (color by mouse_id)\n",
    "# Get the first two PCs for initialization\n",
    "from openTSNE import TSNE\n",
    "\n",
    "initial_embedding = X_pca[:, :2]\n",
    "\n",
    "# t-SNE with openTSNE\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    initialization=initial_embedding,  # Initialize with first two PCs\n",
    "    perplexity=50,  # Adjust based on your data\n",
    "    n_jobs=-1,      # Use all available CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "X_tsne = tsne.fit(X_pca)\n",
    "print('X_tsne: ', X_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['tsne_conv2_channel'] = X_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rastermap of channel responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(channel_resp_all.shape) # nmouse, NN, ntrain, 64\n",
    "nmouse, NN, ntrain, nconv2 = channel_resp_all.shape\n",
    "channel_resp_all_flat = channel_resp_all.transpose(0,1,3,2).reshape(nmouse*NN*nconv2, ntrain)\n",
    "channel_resp_all_flat = channel_resp_all_flat / (np.abs(channel_resp_all_flat).max(axis=1)[:, None] + 1e-6)\n",
    "mouse_ids = np.zeros(Wc_all.shape)\n",
    "for i in range(nmouse):\n",
    "    mouse_ids[i] = i\n",
    "mouse_ids_flat = mouse_ids.flatten()\n",
    "Wc_all_flat = Wc_all.flatten()\n",
    "print(channel_resp_all_flat.shape, mouse_ids_flat.shape, Wc_all_flat.shape)\n",
    "valid_idxes = np.where(np.abs(Wc_all_flat) > 0.01)[0]\n",
    "Wc_all_flat = Wc_all_flat[valid_idxes]\n",
    "channel_resp_all_flat = channel_resp_all_flat[valid_idxes]\n",
    "mouse_ids_flat = mouse_ids_flat[valid_idxes]\n",
    "print(channel_resp_all_flat.shape, mouse_ids_flat.shape, Wc_all_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rastermap of responses\n",
    "# with a plot with mouse id\n",
    "# 600 neurons, show everything binned\n",
    "from scipy.stats import zscore\n",
    "from rastermap import Rastermap\n",
    "channel_resp = zscore(channel_resp_all_flat, axis=1)\n",
    "n_neurons, n_stim = channel_resp.shape\n",
    "n_bins = 500\n",
    "bin_size = n_neurons // n_bins\n",
    "model = Rastermap(n_clusters=100, # number of clusters to compute\n",
    "                  n_PCs=200, # number of PCs to use\n",
    "                  locality=0.5, # locality in sorting to find sequences (this is a value from 0-1)\n",
    "                  # grid_upsample=10, # default value, 10 is good for large recordings\n",
    "                  bin_size=bin_size\n",
    "                ).fit(channel_resp)\n",
    "\n",
    "y = model.embedding # neurons x 1\n",
    "isort = model.isort\n",
    "\n",
    "x = model.X_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['rastermap_x'] = x\n",
    "data_dict['rastermap_channel_resp'] = channel_resp\n",
    "data_dict['rastermap_isort'] = isort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import sup_figure\n",
    "root = './outputs'\n",
    "sup_figure.figure_conv2_cluster(data_dict, root)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure 13: example neurons visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sup_figure\n",
    "from fig_utils import *\n",
    "save_path = './outputs'\n",
    "sup_figure.figure9(data_dict, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(f'supfig_results.npz', **data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
